from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,  GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error, classification_report
from xgboost import XGBClassifier
from plotting import plot_classification_report
from yellowbrick.model_selection import validation_curve
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import time


def classifying(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

    classifiers = {
        "Random Forest": RandomForestClassifier(max_depth=10, min_samples_leaf=2),
        # "SVM": SVC(random_state=0, C=10),
        # "GradientBoosting": GradientBoostingClassifier(n_estimators=50),
        # "AdaBoost": AdaBoostClassifier(n_estimators=100),
        # "Decision Tree": DecisionTreeClassifier(random_state=1),
        # "Logistic Regression": LogisticRegression(random_state=1, max_iter=500),
        # "Naive Bayes": GaussianNB(),
        # "XGBoost": XGBClassifier(),
        # "KNN": KNeighborsClassifier(),
    }

    for classifier in classifiers.keys():
        clf = classifiers[classifier]
        clf.fit(x_train, y_train)
        predict = clf.predict(x_test)
        print(f'{classifier} classification result is:')
        print(classification_report(y_test, predict))
        print(validation_curve(clf, x, y, param_name="n_estimators", n_jobs=-1,
                               param_range=[50, 100, 200, 500, 1000], cv=5, scoring="accuracy"))



def random_forest_parameters_tuning(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
    # Number of trees in random forest
    n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]

    # Number of features to consider at every split
    max_features = ['auto', 'sqrt']

    # Maximum number of levels in tree
    max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
    max_depth.append(None)

    # Minimum number of samples required to split a node
    min_samples_split = [2, 5, 10]

    # Minimum number of samples required at each leaf node
    min_samples_leaf = [1, 2, 4]

    # Method of selecting samples for training each tree
    bootstrap = [True, False]

    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}
    print(random_grid)

    # Use the random grid to search for best hyperparameters
    # First create the base model to tune
    rf = RandomForestClassifier()
    # Random search of parameters, using 3 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2,
                                   random_state=42, n_jobs=-1)
    # Fit the random search model
    rf_random.fit(x_train, y_train)

    print(rf_random.best_params_)

    predict = rf_random.predict(x_test)
    print(classification_report(y_test, predict))
    print(f'tuned random forest classification result is: {mean_squared_error(y_test, predict)}')


def svm_parameters_tuning(X, y):
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    hyper = {#'C': [0.05, 0.1, 0.2, 0.4, 0.5, 0.7, 0.8, 1],
             #'gamma': ['scale', 'auto', 0.1, 0.2, 0.4, 0.6, 0.8, 1.0],
             #'kernel': ['rbf', 'linear']
             }

    grid = RandomizedSearchCV(estimator=SVC(), param_grid=hyper, n_iter=100, verbose=True)

    grid.fit(x_train, y_train)
    print(grid.best_score_)
    print(grid.best_estimator_)

    predict = grid.predict(x_test)
    classificationReport = classification_report(y_test, predict)
    plot_classification_report(classificationReport)
    plt.savefig('test_plot_classif_report.png', dpi=200, format='png', bbox_inches='tight')
    plt.close()


def best_random_forest(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
    forest_model = RandomForestClassifier(n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features='auto', max_depth=50, bootstrap=False)
    forest_model.fit(x_train, y_train)

    predict = forest_model.predict(x_test)
    print(classification_report(y_test, predict))
    print(f'random forest result is: {mean_squared_error(y_test, predict)}')

    class_report = classification_report(y_test, predict)
    plot_classification_report(class_report)
    plt.savefig('test_plot_classif_report.png', dpi=200, format='png', bbox_inches='tight')
    plt.close()


def malware_detect():
    start = time.time()
    dataset = pd.read_csv("ClaMP_Raw-5184.csv")
    dataset = dataset[dataset < 2**31].fillna(0)

    features = dataset.columns[:-1]
    x = dataset.loc[:, features]
    y = dataset["class"]

    classifying(x, y)

    end = time.time()
    print(f'time passed is: {end - start}')


if __name__ == '__main__':
    malware_detect()
